{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSI 4142 Data Science \n",
    "## Assignment 2 - Data Cleaning\n",
    "\n",
    "### Identification\n",
    "\n",
    "Name: Eli Wynn<br/>\n",
    "Student Number: 300248135\n",
    "\n",
    "Name: Jack Snelgrove<br/>\n",
    "Student Number: 300247435\n",
    "\n",
    "\n",
    "Our datasets have been uploaded from the public repository:\n",
    "\n",
    "- [github.com/eli-wynn/Datasets](https://github.com/eli-wynn/Datasets)\n",
    "\n",
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix  = \"https://raw.githubusercontent.com/eli-wynn/Datasets/refs/heads/main/netflix_titles.csv\"\n",
    "netflixData = pd.read_csv(netflix)\n",
    "startup = \"https://raw.githubusercontent.com/eli-wynn/Datasets/refs/heads/main/startup.csv\"\n",
    "startupData = pd.read_csv(startup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netflix Shows Dataset\n",
    "\n",
    "#### Dataset Overview\n",
    "The **Netflix Shows Dataset** was created by **Shivam Bansal**. It contains a comprehensive list of TV shows and movies available on Netflix, including various metadata attributes such as title, type, director, cast, release year, and more. This dataset is useful for data analysis, visualization, and machine learning applications related to content recommendations.\n",
    "\n",
    "#### Dataset Shape\n",
    "- **Rows:** 8,801\n",
    "- **Columns:** 12\n",
    "\n",
    "#### Features and Descriptions\n",
    "Below is a list of features included in the dataset along with their descriptions:\n",
    "\n",
    "##### 1. `show_id` (Categorical)\n",
    "   - Unique identifier for each show or movie.\n",
    "\n",
    "##### 2. `type` (Categorical)\n",
    "   - Specifies whether the content is a **Movie** or a **TV Show**.\n",
    "\n",
    "##### 3. `title` (Categorical)\n",
    "   - Title of the movie or TV show.\n",
    "\n",
    "##### 4. `director` (Categorical, Can be NaN)\n",
    "   - Name(s) of the director(s) of the content.\n",
    "\n",
    "##### 5. `cast` (Categorical, Can be NaN)\n",
    "   - List of main cast members.\n",
    "\n",
    "##### 6. `country` (Categorical, Can be NaN)\n",
    "   - Country where the movie or show was produced.\n",
    "\n",
    "##### 7. `date_added` (Categorical, Can be NaN)\n",
    "   - Date when the content was added to Netflix.\n",
    "\n",
    "##### 8. `release_year` (Numerical)\n",
    "   - Year when the movie or TV show was released.\n",
    "\n",
    "##### 9. `rating` (Categorical)\n",
    "   - Content rating (e.g., PG-13, TV-MA, R, G).\n",
    "\n",
    "##### 10. `duration` (Categorical)\n",
    "   - Duration of the movie (in minutes) or number of seasons (for TV shows).\n",
    "\n",
    "##### 11. `listed_in` (Categorical)\n",
    "   - Categories or genres the content falls under.\n",
    "\n",
    "##### 12. `description` (Categorical)\n",
    "   - Brief summary or description of the movie or TV show.\n",
    "\n",
    "This dataset provides valuable insights into the type of content available on Netflix and allows for various analytical explorations such as trends in movie releases, genre distributions, and rating patterns.\n"
    "## Startup Dataset\n",
    "\n",
    "### Dataset overview \n",
    "This dataset contains 5,000 records of startup growth and investment data across various industries. It provides insights into funding trends, valuation, and investor activity for startups globally. The dataset is ideal for machine learning models, data analysis, and trend predictions in the startup ecosystem.\n",
    "\n",
    "### Dataset Shape\n",
    "- Rows: 5000\n",
    "- Columns: 9\n",
    "\n",
    "### Features and Descriptions \n",
    "\n",
    "##### 1. `Startup Name` (Categorical)\n",
    "- Name of the startup\n",
    "\n",
    "##### 2. `Industry` (Categorical)\n",
    "- The industry sector (e.g., AI, Fintech, HealthTech, etc.)\n",
    "\n",
    "##### 3. `Funding Rounds` (Categorical)\n",
    "- Number of funding rounds received by the startup\n",
    "\n",
    "##### 4. `Investment Amount (USD)` (Numerical)\n",
    "- Total investment received in USD\n",
    "\n",
    "##### 5. `Valuation (USD)` (Numerical)\n",
    "- Estimated company valuation in USD\n",
    "\n",
    "##### 6. `Number of Investors` (Numerical)\n",
    "- Total number of investors backing the startup\n",
    "  \n",
    "##### 7. `Country` (Categorical)\n",
    "- Country where the startup is based\n",
    "  \n",
    "##### 8. `Year Founded` (Numerical)\n",
    "- Year when the startup was founded\n",
    "\n",
    "##### 9. `Growth Rate (%)` (Numerical)\n",
    "- Annual growth rate percentage\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data Checker\n",
    "\n",
    "#### Data Type Error\n",
    "\n",
    "A data type error occurs when the the data entered into a column doesnt match the data type assigned to that column. There are zero datatype errors in the Netflix dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "intCol = ['release_year']\n",
    "stringCols = ['show_id', 'type', 'title', 'director', 'cast', 'country', 'rating', 'duration', 'listed_in', 'description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checker Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in intCol:\n",
    "    netflixData[col] = pd.to_numeric(netflixData[col], errors='coerce')\n",
    "\n",
    "for col in stringCols:\n",
    "    invalid_strings = netflixData[~netflixData[col].astype(str).apply(lambda x: isinstance(x, str))]\n",
    "    if not invalid_strings.empty:\n",
    "        print(f\"\\nPossible non-string values in '{col}':\\n\", invalid_strings.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible data type errors:\n",
      " Empty DataFrame\n",
      "Columns: [show_id, type, title, director, cast, country, date_added, release_year, rating, duration, listed_in, description]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Find rows where conversion resulted in NaN (potential type errors)\n",
    "type_errors = netflixData[netflixData[intCol].isna().any(axis=1)]\n",
    "print(\"Possible data type errors:\\n\", type_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Range Errors\n",
    "\n",
    "Searches for errors where the data is outside acceptable range (e.g. season -1 or release date prior to 1930)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "releaseParam = [1925, 2025]\n",
    "durationParams = [0, 300] #split on space and make sure first item in array is >0 <300\n",
    "dateAdded = [2007, 2025] #just look at year "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checker Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "releaseErrors = netflixData[(netflixData['release_year'] < releaseParam[0]) | (netflixData['release_year'] > releaseParam[1])]\n",
    "\n",
    "netflixData['duration_split'] = netflixData['duration'].str.split(\" \").str[0]  # Extract number part\n",
    "netflixData['duration_split'] = pd.to_numeric(netflixData['duration_split'], errors='coerce')  # Convert to int\n",
    "# Identify invalid durations\n",
    "durationErrors = netflixData[(netflixData['duration_split'] <= durationParams[0]) | (netflixData['duration_split'] >= durationParams[1])]\n",
    "\n",
    "netflixData['date_added'] = pd.to_datetime(netflixData['date_added'], errors='coerce')\n",
    "netflixData['year_added'] = netflixData['date_added'].dt.year #take just year value, other date errors will be caught in format error below\n",
    "# Identify invalid date_added values\n",
    "dateAddedErrors = netflixData[(netflixData['year_added'] < dateAdded[0]) | (netflixData['year_added'] > dateAdded[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "Only one duration error, a black mirror episode longer than 300 minutes, don't know whether it is just an abnormally long episode outside my set parameters or needs to be cleaned up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Release Year Errors:\n",
      " Empty DataFrame\n",
      "Columns: [title, release_year]\n",
      "Index: []\n",
      "\n",
      "Duration Errors:\n",
      "      show_id                       title duration  duration_split\n",
      "4253   s4254  Black Mirror: Bandersnatch  312 min           312.0\n",
      "\n",
      "Date Added Errors:\n",
      " Empty DataFrame\n",
      "Columns: [title, date_added, year_added]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRelease Year Errors:\\n\", releaseErrors[['title', 'release_year']].head(5))\n",
    "print(\"\\nDuration Errors:\\n\", durationErrors[['show_id', 'title', 'duration', 'duration_split']].head(5))\n",
    "print(\"\\nDate Added Errors:\\n\", dateAddedErrors[['title', 'date_added', 'year_added']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format Errors\n",
    "\n",
    "Checks for errors with the formatting of the data, e.g. date being DD-MM-YYYY instead of YYYY first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateCol = ['date_added'] #make sure date is correct format\n",
    "showCol = ['show_id'] #make sure it is s### format\n",
    "durationCol = ['duration'] #make sure duration is number followed by either \"min\" or \"Season\" or \"Seasons\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checker Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dateCol:\n",
    "    netflixData[col] = pd.to_datetime(netflixData[col], errors='coerce')\n",
    "invalid_dates = netflixData[netflixData[dateCol].isna().any(axis=1)]\n",
    "\n",
    "brokenID = netflixData[~netflixData['show_id'].astype(str).str.match(r\"^s\\d{1,4}$\", na=False)] #finds all ids that don't match format of s followed by 1-4 #'s\n",
    "brokenDuration = netflixData[~netflixData['duration'].astype(str).str.match(r\"^\\d+\\s(min|Season|Seasons)$\", na=False)] #finds all id's that don't match digits then space then seasons, season or min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "Louis C.K. titles have the durations in the ratings column for some reason - formatting is technically correct though\n",
    "<br> No show ID errors\n",
    "<br> There are a few date formatting errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Possible date format errors:\n",
      "                                             title date_added\n",
      "6066  A Young Doctor's Notebook and Other Stories        NaT\n",
      "6079                              Abnormal Summit        NaT\n",
      "6174              Anthony Bourdain: Parts Unknown        NaT\n",
      "6177                                     忍者ハットリくん        NaT\n",
      "6213                                Bad Education        NaT\n",
      "\n",
      "Show ID Format Errors:\n",
      " Empty DataFrame\n",
      "Columns: [show_id, title]\n",
      "Index: []\n",
      "\n",
      "Duration Format Errors:\n",
      "      duration                                 title\n",
      "5541      NaN                       Louis C.K. 2017\n",
      "5794      NaN                 Louis C.K.: Hilarious\n",
      "5813      NaN  Louis C.K.: Live at the Comedy Store\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPossible date format errors:\\n\", invalid_dates[['title', 'date_added']].head(5))\n",
    "print(\"\\nShow ID Format Errors:\\n\", brokenID[['show_id', 'title']].head(5))\n",
    "print(\"\\nDuration Format Errors:\\n\", brokenDuration[['duration', 'title']].head(5)) #Louis C.k. durations are in rating column for some reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency Errors\n",
    "\n",
    "a type of logical check that ensures data is entered in a\n",
    "logically consistent manner. In this data a consistensy error could be checking that movies all have their duration in minutes and shows in seasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters changed below by updating movie or tv show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checker Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieErr = netflixData[(netflixData['type'] == 'Movie')&~netflixData['duration'].astype(str).str.match(r\"^\\d+\\smin$\", na=False)]\n",
    "tvErr = netflixData[(netflixData['type'] == 'TV Show')&~netflixData['duration'].astype(str).str.match(r\"^\\d+\\s(Season|Seasons)$\", na=False)] #match all tv show types and regex to ensure consistent season/seasons format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "No errors in the consistency just the same duration absences discovered above in the louis C.K. stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Incosistent Movie Durations: \n",
      "                                      title   type duration\n",
      "5541                       Louis C.K. 2017  Movie      NaN\n",
      "5794                 Louis C.K.: Hilarious  Movie      NaN\n",
      "5813  Louis C.K.: Live at the Comedy Store  Movie      NaN\n",
      "\n",
      "Incosistent Show Durations: \n",
      " Empty DataFrame\n",
      "Columns: [title, type, duration]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nIncosistent Movie Durations: \\n\", movieErr[['title', 'type', 'duration']].head(5))\n",
    "print(\"\\nIncosistent Show Durations: \\n\", tvErr[['title', 'type', 'duration']].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniqueness Errors\n",
    "\n",
    "Ensure that there are no duplicate values, The title column is the only applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquenessParams = ['show_id', 'title', 'release_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checker Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupeShowID = netflixData[netflixData.duplicated(subset=[uniquenessParams[0]], keep=False)] #showID duplicate\n",
    "\n",
    "#Checking for duplicate titles isn't valid because two movies can have the same names so need to compare with release year as well\n",
    "dupeTitle = netflixData[netflixData.duplicated(subset=[uniquenessParams[1], uniquenessParams[2]], keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate Show IDs: \n",
      " Empty DataFrame\n",
      "Columns: [show_id]\n",
      "Index: []\n",
      "\n",
      "Duplicate titles: \n",
      " Empty DataFrame\n",
      "Columns: [title, release_year]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDuplicate Show IDs: \\n\", dupeShowID[[uniquenessParams[0]]].head(5))\n",
    "print(\"\\nDuplicate titles: \\n\", dupeTitle[[uniquenessParams[1], uniquenessParams[2]]].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Presence Errors\n",
    "\n",
    "A presence error is when a mandatory value is left blank, in this case you could argue that every column is necessary but title, id and type are the most important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mandatoryVals = ['show_id', 'title', 'release_year', 'type', 'duration']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checker Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingValues = netflixData[netflixData[[mandatoryVals[0], mandatoryVals[1], mandatoryVals[2], mandatoryVals[3], mandatoryVals[4]]].isnull().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing mandatory values: \n",
      "      show_id                                 title  release_year   type  \\\n",
      "5541   s5542                       Louis C.K. 2017          2017  Movie   \n",
      "5794   s5795                 Louis C.K.: Hilarious          2010  Movie   \n",
      "5813   s5814  Louis C.K.: Live at the Comedy Store          2015  Movie   \n",
      "\n",
      "     duration  \n",
      "5541      NaN  \n",
      "5794      NaN  \n",
      "5813      NaN  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing mandatory values: \\n\", missingValues[['show_id', 'title', \n",
    "                                                'release_year', 'type', 'duration']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Length Errors\n",
    "\n",
    "Errors in which the length of a parameter is different then the norm. e.g. A title that is 2000 characters long is very likely a length error and an incorrectly input piece of data because 2000 characters is unreasonable to say"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "titleSizeParam = [1,200]\n",
    "relYearParam = [4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checker Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "titleLen = netflixData[(netflixData['title'].str.len() < titleSizeParam[0]) | (netflixData['title'].str.len() > titleSizeParam[1])]\n",
    "relYearLen = netflixData[netflixData['release_year'].astype(str).str.len() == releaseParam[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "Zero length errors are present given the above criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title length errors:\n",
      "Empty DataFrame\n",
      "Columns: [title, show_id]\n",
      "Index: []\n",
      "\n",
      "Release year length errors found:\n",
      "Empty DataFrame\n",
      "Columns: [title, show_id, release_year]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTitle length errors:\")\n",
    "print(titleLen[['title', 'show_id']].head())\n",
    "print(\"\\nRelease year length errors found:\")\n",
    "print(relYearLen[['title', 'show_id', 'release_year']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lookup Errors\n",
    "\n",
    "Errors which are caused by a value being outside the correct set of values. For example, having a made up month in a month of release. In this dataset lookup errors can be present in the country value, rating or type columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Country dictionary sourced from: https://stackoverflow.com/questions/41245330/check-if-a-country-entered-is-one-of-the-countries-of-the-world\n",
    "Country = [\n",
    "    ('US', 'United States'),\n",
    "    ('AF', 'Afghanistan'),\n",
    "    ('AL', 'Albania'),\n",
    "    ('DZ', 'Algeria'),\n",
    "    ('AS', 'American Samoa'),\n",
    "    ('AD', 'Andorra'),\n",
    "    ('AO', 'Angola'),\n",
    "    ('AI', 'Anguilla'),\n",
    "    ('AQ', 'Antarctica'),\n",
    "    ('AG', 'Antigua And Barbuda'),\n",
    "    ('AR', 'Argentina'),\n",
    "    ('AM', 'Armenia'),\n",
    "    ('AW', 'Aruba'),\n",
    "    ('AU', 'Australia'),\n",
    "    ('AT', 'Austria'),\n",
    "    ('AZ', 'Azerbaijan'),\n",
    "    ('BS', 'Bahamas'),\n",
    "    ('BH', 'Bahrain'),\n",
    "    ('BD', 'Bangladesh'),\n",
    "    ('BB', 'Barbados'),\n",
    "    ('BY', 'Belarus'),\n",
    "    ('BE', 'Belgium'),\n",
    "    ('BZ', 'Belize'),\n",
    "    ('BJ', 'Benin'),\n",
    "    ('BM', 'Bermuda'),\n",
    "    ('BT', 'Bhutan'),\n",
    "    ('BO', 'Bolivia'),\n",
    "    ('BA', 'Bosnia And Herzegowina'),\n",
    "    ('BW', 'Botswana'),\n",
    "    ('BV', 'Bouvet Island'),\n",
    "    ('BR', 'Brazil'),\n",
    "    ('BN', 'Brunei Darussalam'),\n",
    "    ('BG', 'Bulgaria'),\n",
    "    ('BF', 'Burkina Faso'),\n",
    "    ('BI', 'Burundi'),\n",
    "    ('KH', 'Cambodia'),\n",
    "    ('CM', 'Cameroon'),\n",
    "    ('CA', 'Canada'),\n",
    "    ('CV', 'Cape Verde'),\n",
    "    ('KY', 'Cayman Islands'),\n",
    "    ('CF', 'Central African Rep'),\n",
    "    ('TD', 'Chad'),\n",
    "    ('CL', 'Chile'),\n",
    "    ('CN', 'China'),\n",
    "    ('CX', 'Christmas Island'),\n",
    "    ('CC', 'Cocos Islands'),\n",
    "    ('CO', 'Colombia'),\n",
    "    ('KM', 'Comoros'),\n",
    "    ('CG', 'Congo'),\n",
    "    ('CK', 'Cook Islands'),\n",
    "    ('CR', 'Costa Rica'),\n",
    "    ('CI', 'Cote D`ivoire'),\n",
    "    ('HR', 'Croatia'),\n",
    "    ('CU', 'Cuba'),\n",
    "    ('CY', 'Cyprus'),\n",
    "    ('CZ', 'Czech Republic'),\n",
    "    ('DK', 'Denmark'),\n",
    "    ('DJ', 'Djibouti'),\n",
    "    ('DM', 'Dominica'),\n",
    "    ('DO', 'Dominican Republic'),\n",
    "    ('TP', 'East Timor'),\n",
    "    ('EC', 'Ecuador'),\n",
    "    ('EG', 'Egypt'),\n",
    "    ('SV', 'El Salvador'),\n",
    "    ('GQ', 'Equatorial Guinea'),\n",
    "    ('ER', 'Eritrea'),\n",
    "    ('EE', 'Estonia'),\n",
    "    ('ET', 'Ethiopia'),\n",
    "    ('FK', 'Falkland Islands (Malvinas)'),\n",
    "    ('FO', 'Faroe Islands'),\n",
    "    ('FJ', 'Fiji'),\n",
    "    ('FI', 'Finland'),\n",
    "    ('FR', 'France'),\n",
    "    ('GF', 'French Guiana'),\n",
    "    ('PF', 'French Polynesia'),\n",
    "    ('TF', 'French S. Territories'),\n",
    "    ('GA', 'Gabon'),\n",
    "    ('GM', 'Gambia'),\n",
    "    ('GE', 'Georgia'),\n",
    "    ('DE', 'Germany'),\n",
    "    ('GH', 'Ghana'),\n",
    "    ('GI', 'Gibraltar'),\n",
    "    ('GR', 'Greece'),\n",
    "    ('GL', 'Greenland'),\n",
    "    ('GD', 'Grenada'),\n",
    "    ('GP', 'Guadeloupe'),\n",
    "    ('GU', 'Guam'),\n",
    "    ('GT', 'Guatemala'),\n",
    "    ('GN', 'Guinea'),\n",
    "    ('GW', 'Guinea-bissau'),\n",
    "    ('GY', 'Guyana'),\n",
    "    ('HT', 'Haiti'),\n",
    "    ('HN', 'Honduras'),\n",
    "    ('HK', 'Hong Kong'),\n",
    "    ('HU', 'Hungary'),\n",
    "    ('IS', 'Iceland'),\n",
    "    ('IN', 'India'),\n",
    "    ('ID', 'Indonesia'),\n",
    "    ('IR', 'Iran'),\n",
    "    ('IQ', 'Iraq'),\n",
    "    ('IE', 'Ireland'),\n",
    "    ('IL', 'Israel'),\n",
    "    ('IT', 'Italy'),\n",
    "    ('JM', 'Jamaica'),\n",
    "    ('JP', 'Japan'),\n",
    "    ('JO', 'Jordan'),\n",
    "    ('KZ', 'Kazakhstan'),\n",
    "    ('KE', 'Kenya'),\n",
    "    ('KI', 'Kiribati'),\n",
    "    ('KP', 'Korea (North)'),\n",
    "    ('KR', 'Korea (South)'),\n",
    "    ('KW', 'Kuwait'),\n",
    "    ('KG', 'Kyrgyzstan'),\n",
    "    ('LA', 'Laos'),\n",
    "    ('LV', 'Latvia'),\n",
    "    ('LB', 'Lebanon'),\n",
    "    ('LS', 'Lesotho'),\n",
    "    ('LR', 'Liberia'),\n",
    "    ('LY', 'Libya'),\n",
    "    ('LI', 'Liechtenstein'),\n",
    "    ('LT', 'Lithuania'),\n",
    "    ('LU', 'Luxembourg'),\n",
    "    ('MO', 'Macau'),\n",
    "    ('MK', 'Macedonia'),\n",
    "    ('MG', 'Madagascar'),\n",
    "    ('MW', 'Malawi'),\n",
    "    ('MY', 'Malaysia'),\n",
    "    ('MV', 'Maldives'),\n",
    "    ('ML', 'Mali'),\n",
    "    ('MT', 'Malta'),\n",
    "    ('MH', 'Marshall Islands'),\n",
    "    ('MQ', 'Martinique'),\n",
    "    ('MR', 'Mauritania'),\n",
    "    ('MU', 'Mauritius'),\n",
    "    ('YT', 'Mayotte'),\n",
    "    ('MX', 'Mexico'),\n",
    "    ('FM', 'Micronesia'),\n",
    "    ('MD', 'Moldova'),\n",
    "    ('MC', 'Monaco'),\n",
    "    ('MN', 'Mongolia'),\n",
    "    ('MS', 'Montserrat'),\n",
    "    ('MA', 'Morocco'),\n",
    "    ('MZ', 'Mozambique'),\n",
    "    ('MM', 'Myanmar'),\n",
    "    ('NA', 'Namibia'),\n",
    "    ('NR', 'Nauru'),\n",
    "    ('NP', 'Nepal'),\n",
    "    ('NL', 'Netherlands'),\n",
    "    ('AN', 'Netherlands Antilles'),\n",
    "    ('NC', 'New Caledonia'),\n",
    "    ('NZ', 'New Zealand'),\n",
    "    ('NI', 'Nicaragua'),\n",
    "    ('NE', 'Niger'),\n",
    "    ('NG', 'Nigeria'),\n",
    "    ('NU', 'Niue'),\n",
    "    ('NF', 'Norfolk Island'),\n",
    "    ('MP', 'Northern Mariana Islands'),\n",
    "    ('NO', 'Norway'),\n",
    "    ('OM', 'Oman'),\n",
    "    ('PK', 'Pakistan'),\n",
    "    ('PW', 'Palau'),\n",
    "    ('PA', 'Panama'),\n",
    "    ('PG', 'Papua New Guinea'),\n",
    "    ('PY', 'Paraguay'),\n",
    "    ('PE', 'Peru'),\n",
    "    ('PH', 'Philippines'),\n",
    "    ('PN', 'Pitcairn'),\n",
    "    ('PL', 'Poland'),\n",
    "    ('PT', 'Portugal'),\n",
    "    ('PR', 'Puerto Rico'),\n",
    "    ('QA', 'Qatar'),\n",
    "    ('RE', 'Reunion'),\n",
    "    ('RO', 'Romania'),\n",
    "    ('RU', 'Russian Federation'),\n",
    "    ('RW', 'Rwanda'),\n",
    "    ('KN', 'Saint Kitts And Nevis'),\n",
    "    ('LC', 'Saint Lucia'),\n",
    "    ('VC', 'St Vincent/Grenadines'),\n",
    "    ('WS', 'Samoa'),\n",
    "    ('SM', 'San Marino'),\n",
    "    ('ST', 'Sao Tome'),\n",
    "    ('SA', 'Saudi Arabia'),\n",
    "    ('SN', 'Senegal'),\n",
    "    ('SC', 'Seychelles'),\n",
    "    ('SL', 'Sierra Leone'),\n",
    "    ('SG', 'Singapore'),\n",
    "    ('SK', 'Slovakia'),\n",
    "    ('SI', 'Slovenia'),\n",
    "    ('SB', 'Solomon Islands'),\n",
    "    ('SO', 'Somalia'),\n",
    "    ('ZA', 'South Africa'),\n",
    "    ('ES', 'Spain'),\n",
    "    ('LK', 'Sri Lanka'),\n",
    "    ('SH', 'St. Helena'),\n",
    "    ('PM', 'St.Pierre'),\n",
    "    ('SD', 'Sudan'),\n",
    "    ('SR', 'Suriname'),\n",
    "    ('SZ', 'Swaziland'),\n",
    "    ('SE', 'Sweden'),\n",
    "    ('CH', 'Switzerland'),\n",
    "    ('SY', 'Syrian Arab Republic'),\n",
    "    ('TW', 'Taiwan'),\n",
    "    ('TJ', 'Tajikistan'),\n",
    "    ('TZ', 'Tanzania'),\n",
    "    ('TH', 'Thailand'),\n",
    "    ('TG', 'Togo'),\n",
    "    ('TK', 'Tokelau'),\n",
    "    ('TO', 'Tonga'),\n",
    "    ('TT', 'Trinidad And Tobago'),\n",
    "    ('TN', 'Tunisia'),\n",
    "    ('TR', 'Turkey'),\n",
    "    ('TM', 'Turkmenistan'),\n",
    "    ('TV', 'Tuvalu'),\n",
    "    ('UG', 'Uganda'),\n",
    "    ('UA', 'Ukraine'),\n",
    "    ('AE', 'United Arab Emirates'),\n",
    "    ('UK', 'United Kingdom'),\n",
    "    ('UY', 'Uruguay'),\n",
    "    ('UZ', 'Uzbekistan'),\n",
    "    ('VU', 'Vanuatu'),\n",
    "    ('VA', 'Vatican City State'),\n",
    "    ('VE', 'Venezuela'),\n",
    "    ('VN', 'Viet Nam'),\n",
    "    ('VG', 'Virgin Islands (British)'),\n",
    "    ('VI', 'Virgin Islands (U.S.)'),\n",
    "    ('EH', 'Western Sahara'),\n",
    "    ('YE', 'Yemen'),\n",
    "    ('YU', 'Yugoslavia'),\n",
    "    ('ZR', 'Zaire'),\n",
    "    ('ZM', 'Zambia'),\n",
    "    ('ZW', 'Zimbabwe')\n",
    "]\n",
    "\n",
    "validCountries = {name for code, name in Country}\n",
    "validRatings = {\"G\", \"PG\", \"PG-13\", \"R\", \"NC-17\", \"TV-Y\", \"TV-Y7\", \"TV-G\", \"TV-PG\", \"TV-14\", \"TV-MA\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checker Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryError = netflixData[netflixData['country'].notna() & ~netflixData['country'].isin(validCountries)]\n",
    "ratingError = netflixData[netflixData['rating'].notna() & ~netflixData['rating'].isin(validRatings)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "There are many country lookup errors in the data because my check doesn't account for multiple values inside the country column. There are still errors with the wrong stuff being present in the wrong columns in the louis CK ones as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Country error: \n",
      "                   title show_id  \\\n",
      "7               Sankofa      s8   \n",
      "12         Je Suis Karl     s13   \n",
      "29             Paranoia     s30   \n",
      "38  Birth of the Dragon     s39   \n",
      "46           Safe House     s47   \n",
      "\n",
      "                                              country  \n",
      "7   United States, Ghana, Burkina Faso, United Kin...  \n",
      "12                            Germany, Czech Republic  \n",
      "29                       United States, India, France  \n",
      "38                       China, Canada, United States  \n",
      "46                 South Africa, United States, Japan  \n",
      "\n",
      "Rating error: \n",
      "                                      title show_id  rating\n",
      "5541                       Louis C.K. 2017   s5542  74 min\n",
      "5794                 Louis C.K.: Hilarious   s5795  84 min\n",
      "5813  Louis C.K.: Live at the Comedy Store   s5814  66 min\n",
      "5971                              (T)ERROR   s5972      NR\n",
      "5987                            13 Cameras   s5988      NR\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCountry error: \\n\", countryError[['title', 'show_id', 'country']].head(5))\n",
    "print(\"\\nRating error: \\n\", ratingError[['title', 'show_id', 'rating']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact Duplicate Errors\n",
    "Exact duplicates occur when multiple rows in the dataset contain the exact same values \n",
    "for all attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_subset = ['title', 'show_id', 'country']  # Columns to check for exact duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checker Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exactDuplicates = netflixData[netflixData.duplicated(subset=duplicate_subset)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "There are no exact duplicates in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExact Duplicate Errors: \\n\", exactDuplicates[duplicate_subset].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Near Duplicate Errors\n",
    "\n",
    "Near duplicates are rows that are very similar but have slight differences. This may \n",
    "occur due to typos, inconsistent data formatting, or variations in missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I decided to assume that the titles were correct and am using it as a key to look for near duplicates in comparison\n",
    "#i.e. duplicates where title matches but other columns differ\n",
    "titleCol = 'title'\n",
    "comparisonCols = ['show_id', 'country', 'rating', 'director', 'cast', 'release_year']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checker Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearDuplicates = netflixData[netflixData.duplicated(subset=[titleCol], keep=False)]  # Find rows with duplicate titles\n",
    "\n",
    "nearDuplicateErrors = nearDuplicates[nearDuplicates.apply(\n",
    "    lambda row: any(row[comparisonCols] != nearDuplicates[comparisonCols].iloc[0]), #look for differences in comparison columns\n",
    "    axis=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "There are no near duplicates with the same title but different values in the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNear Duplicate Errors: \\n\", nearDuplicateErrors[comparisonCols + [titleCol]].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputations\n",
    "\n",
    "#### Test #1\n",
    "\n",
    "a) Column: Funding Rounds - Numerical<br>\n",
    "\n",
    "\n",
    "b) Remove 10% of cells in column 'Funding Rounds' - MCAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startupDataMissingValues = startupData.copy()\n",
    "\n",
    "#Determines how many rows would be 10% of the total\n",
    "num_rows = startupDataMissingValues.shape[0]\n",
    "num_missing = int(num_rows * 0.1)  \n",
    "\n",
    "#Randomly selects 10% of rows \n",
    "missing_indices = np.random.choice(startupDataMissingValues.index, size=num_missing, replace=False)\n",
    "\n",
    "#Saves the original values before setting them to null\n",
    "y_true = startupDataMissingValues.loc[missing_indices, 'Funding Rounds']\n",
    "\n",
    "#Replace the selected values with null\n",
    "startupDataMissingValues.loc[missing_indices, 'Funding Rounds'] = np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Method 3: Random Sample Imputation -Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the missing values \n",
    "values = startupDataMissingValues['Funding Rounds'].dropna()\n",
    "\n",
    "#Function to replace missing values with a random values \n",
    "def randomSampleImputation(x):\n",
    "    if pd.isnull(x):\n",
    "        return np.random.choice(values)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "#Applies the Random sameple imputation to the Funding Rounds column\n",
    "startupDataMissingValues['Funding Rounds'] = startupDataMissingValues['Funding Rounds'].apply(randomSampleImputation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Evaluate approach using Mean Squared Error and Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrievs the imputated values to compare to originals\n",
    "y_pred = startupDataMissingValues.loc[missing_indices, 'Funding Rounds']\n",
    "\n",
    "#Calculates metrics\n",
    "mse = np.mean((y_true - y_pred)**2)\n",
    "print('Column Minimum:', startupData['Funding Rounds'].min())\n",
    "print('Column Maximum:', startupData['Funding Rounds'].max())\n",
    "print(\"MSE:\", mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying random sample imputation to the 'Funding Rounds' column, we evaluated the method using the Mean Squared Error (MSE) and Root Mean Squared Error (RMSE). The MSE was calculated to be ~ 15-18, which corresponds to an RMSE of approximately 4 rounds. Considering that the 'Funding Rounds' values range from 1 to 10, this indicates that, on average, our imputed values deviate from the true values by about 4 rounds—nearly 40% of the total range. This substantial error suggests that while random sample imputation may preserve the overall distribution of the observed data, it introduces a significant level of discrepancy, warranting further investigation into alternative imputation strategies.\n",
    "\n",
    "#### Test #2 \n",
    "\n",
    "a) Column: Investment Amount\n",
    "\n",
    "b) If a row has a 'Funding Round' value of 1, it has a 75% chance of having an empty 'Ivestment Amount (USD)' cell, if 'Funding Round' = 2, it has a 30% chance of leaving 'Investment Amount (USD)' blank, and if 'Funding Round = 3, it has a 5% chance of leaving 'Investment Amount (USD) blank. - MAR\n",
    "\n",
    "Assumption: Startups in early funding rounds might not want to disclode their investment amount yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startupDataMissingValues2 = startupData.copy()\n",
    "\n",
    "#assigns a random number to each row between 0-1\n",
    "random_numbers = np.random.rand(startupDataMissingValues2.shape[0])\n",
    "\n",
    "#Sets the respective probabilities for each funding round values\n",
    "prob_1 = 0.75\n",
    "prob_2 = 0.30\n",
    "prob_3 = 0.05\n",
    "\n",
    "#Identitifies the rows that should be removed\n",
    "shouldRemove = (\n",
    "    ((startupDataMissingValues2['Funding Rounds'] == 1) & (random_numbers < prob_1)) |\n",
    "    ((startupDataMissingValues2['Funding Rounds'] == 2) & (random_numbers < prob_2)) |\n",
    "    ((startupDataMissingValues2['Funding Rounds'] == 3) & (random_numbers < prob_3))\n",
    ")\n",
    "\n",
    "#Saves the original values before removing\n",
    "y_true = startupDataMissingValues2.loc[shouldRemove, 'Investment Amount (USD)']\n",
    "\n",
    "#Replaces the identified rows with null\n",
    "startupDataMissingValues2.loc[shouldRemove, 'Investment Amount (USD)'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Method 7: Regression Imputation / Predictive Imputation (Bivariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transforms the column for better results\n",
    "startupDataMissingValues2['Log Investment Amount'] = np.log(startupDataMissingValues2['Investment Amount (USD)'])\n",
    "\n",
    "#Splits the data into training and predicting \n",
    "train_data = startupDataMissingValues2[startupDataMissingValues2['Log Investment Amount'].notnull()]\n",
    "pred_data = startupDataMissingValues2[startupDataMissingValues2['Log Investment Amount'].isnull()]\n",
    "\n",
    "#Defines the feature (Funding Rounds) and the target (Log Investment Amount)\n",
    "X_train = train_data[['Funding Rounds']]\n",
    "y_train = train_data['Log Investment Amount']\n",
    "\n",
    "#Initializes and trains the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Sets the features for the prediction\n",
    "X_pred = pred_data[['Funding Rounds']]\n",
    "\n",
    "#Predicts Log Investment amount\n",
    "y_pred = model.predict(X_pred)\n",
    "\n",
    "# un- log transform the results\n",
    "y_pred  = np.exp(y_pred)\n",
    "\n",
    "# puts the results back into the dataframe\n",
    "startupDataMissingValues2.loc[shouldRemove, 'Investment Amount (USD)'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Evaluate the regression model using MSE and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investment Amount Minimum: 1102610.0\n",
      "Investment Amount Maximum: 4999543707.18\n",
      "MSE: 1.561587135752324e+17\n",
      "RMSE: 395169221.4421973\n",
      "331859979.16259605\n"
     ]
    }
   ],
   "source": [
    "#Matches the shape of y_true\n",
    "y_pred = startupDataMissingValues2.loc[shouldRemove, 'Investment Amount (USD)']\n",
    "\n",
    "#Displays data range\n",
    "print('Investment Amount Minimum:', startupData['Investment Amount (USD)'].min())\n",
    "print('Investment Amount Maximum:', startupData['Investment Amount (USD)'].max())\n",
    "\n",
    "\n",
    "#Calculates Metrics \n",
    "mse = np.mean((y_true - y_pred)**2)\n",
    "print(\"MSE:\", mse)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"MAE:\",mae)\n",
    "\n",
    "#Display histogram\n",
    "sns.histplot(y_true, color='blue', label='Original', kde=True)\n",
    "sns.histplot(y_pred, color='red', label='Imputed', kde=True)\n",
    "plt.legend()\n",
    "plt.title(\"Distribution Comparison: Original vs. Imputed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression imputation performed poorly in predicting missing investment amounts, as reflected in the high error metrics and unnatural imputed distribution. The model failed to capture the range and variability of investment amounts, likely due to the limited number of features and the assumption of linearity. Alternative methods, such as non-linear regression, random forests, or similarity-based (KNN) imputation, could have better results by capturing more complex patterns.\n",
    "\n",
    "#### Test #3\n",
    "\n",
    "a) Column: Valuation (USD)\n",
    "\n",
    "b) Remove all Valuations under $1,000,000,000\n",
    "\n",
    "Assumption: Companies with Valuations under $1,000,000,000 want to keep that information confidential so they do not appear as small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startupDataMissingValues3= startupData.copy()\n",
    "\n",
    "#Finds which rows to remove based on assumption\n",
    "shouldRemove= startupDataMissingValues3['Valuation (USD)']<=1000000000\n",
    "\n",
    "# Saves the original values so they can be used to compare to imputed values\n",
    "y_true = startupDataMissingValues3.loc[shouldRemove, 'Valuation (USD)']\n",
    "\n",
    "#Removes the rows \n",
    "startupDataMissingValues3.loc[shouldRemove, 'Valuation (USD)'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Method 9: Similarity Based Imputation (Multivariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical columns\n",
    "startupDataEncoded = pd.get_dummies(startupDataMissingValues3, columns=['Startup Name', 'Industry', 'Country'])\n",
    "\n",
    "# Identify numerical columns\n",
    "num_cols = ['Funding Rounds', 'Valuation (USD)', 'Number of Investors', 'Year Founded']\n",
    "\n",
    "# Apply scaling only to numerical columns\n",
    "scaler = StandardScaler()\n",
    "startupDataEncoded[num_cols] = scaler.fit_transform(startupDataEncoded[num_cols])\n",
    "\n",
    "# Initialize and apply KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "imputed_data = imputer.fit_transform(startupDataEncoded)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "startupDataImputed = pd.DataFrame(imputed_data, columns=startupDataEncoded.columns, index=startupDataEncoded.index)\n",
    "\n",
    "# Reverse scaling only for numerical columns\n",
    "startupDataImputed[num_cols] = scaler.inverse_transform(startupDataImputed[num_cols])\n",
    "\n",
    "# Assign the imputed values back to the original missing positions\n",
    "startupDataMissingValues3.loc[shouldRemove, 'Valuation (USD)'] = startupDataImputed.loc[startupDataMissingValues3['Valuation (USD)'].isnull(), 'Valuation (USD)']\n",
    "\n",
    "# Retrieve predictions for comparison\n",
    "y_pred = startupDataMissingValues3.loc[shouldRemove, 'Valuation (USD)']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Evaluate the KNN (Similarity Based) Imputer with mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Metrics x\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Histogram with KDE to see the distribution and skew\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(startupData['Valuation (USD)'], bins=50, kde=True)\n",
    "plt.title(\"Histogram of Valuation (USD)\")\n",
    "plt.xlabel(\"Valuation (USD) in tens of billions\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Boxplot to visualize the range and detect outliers\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x=startupData['Valuation (USD)'])\n",
    "plt.title(\"Boxplot of Valuation (USD)\")\n",
    "plt.xlabel(\"Valuation (USD) in tens billions\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN imputation was able to reasonably reconstruct missing valuation data with an average error of ~$65M, which is relatively small in the context of billion-dollar valuations. While this method effectively maintains the overall distribution, some level of bias may persist due to the complete removal of lower valuations. Further improvements could include refining k, adding more predictive features, or considering alternative imputation techniques such as regression or multiple imputation methods.\n",
    "\n",
    "### Conclusion\n",
    "In this assignment we looked at checking the validity of data using 10 different methods and explored methods in impute missing data. \n",
    "\n",
    "-- Elis stuff\n",
    "\n",
    "In the imputation section, we explored simulating missing data of types 'Missing Completely At Random' (MCAR), 'Missing At Random' (MAR), and 'Missing Not At Random' (MNAR). We then took three different approaches to impute the missing data. First, we did a Random Sample Imputation, second we did a Regression Imputation and finally a K Neaest Neighbours approach. To evaluate these methods, we would save the data to be removed before it got removed and compared it the the output of the inputastion methods. The comparing metrics that we used were Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) in combinations with some plots to tell the full story. Although the methods were not perfect, they did a decent job in imputing the missing values with the exception of the Regression imputation whuch wasnt able to find a linear correlation with the given feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "-https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "-https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html\n",
    "-Lecture: Winter2025-CSI4142-Week6-MissingData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Overall the data was very clean already with only a few errors being present with the duration format, stuff not having a date added. There are a lot of issues with the titles with Louis C.K. as the director and items being stored in the wrong columns. There are also a bunch of lookup errors to do with the country of origin, with criteria currently used it just checks whether the string of country is == to the string in the dictionary which fails when there is multiple countries as the string is different even if the country values are the same. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
